---
title: "Case Study 1 - Non-Linear Regression"
author: "Michael Daniel Bigler and Liam Arthur Phan"
date: "`r Sys.Date()`"
output:
  rmdformats::downcute:
    code_folding: hide
---

```{r setup, include=FALSE}

knitr::opts_chunk$set(
	message = FALSE,
	warning = FALSE,
	include = TRUE,
	fig.align = "center")
rm(list = ls())
cat("\014")

```

<break>

> Goal: Estimate an experience curve with non-linear regression analysis, forecast variable cost development based on the estimated regression equation

# Packages

```{r packages}

library(ggplot2)
library(DT)
library(jtools)
library(kableExtra)


```

# Exercise 1

## Data

> Source: **Advanced Data Driven Decision Making** [S401024](https://moodle.unige.ch/course/view.php?id=2675 "2675") - University of Geneva (GSEM)
>
> Marcel Paulssen, Professor\
> Fereshteh Vahidi, Teaching Assistant\
> Anastasia Floru, Teaching Assistant

```{r load data}

df_original <- read.csv('Case_Study_1.csv')
datatable(df_original)

df_log <- df_original
df <- df_original

```

## Scatterplot

```{r Scatter plot}

A <- df$manufacturing_cost[1]
#A <- 2772.258
b <- mean((log(df$manufacturing_cost)-log(A))/log(df$number_of_solar_panels))
#b <- -0.1730591

experience_curve <- function(X, A, b){
  Y <- A*X^b
  return(Y)
}

df$Y_func <- experience_curve(df$number_of_solar_panels, A, b)
  
Plot1 <- ggplot(df, aes(x=number_of_solar_panels, y=manufacturing_cost)) + geom_point(color="#32A287") + theme_minimal() + labs(title = "Experience Curve",subtitle = "Scatterplot", x="Solar Panels", y="Manufacturing Cost", caption = "Data: Case_Study_1.csv") 

Plot1

```

#### Question 1.1

> Does the multiplicative learning model which assumes the following relationship: $$Y = AX^b$$ apply to this data set?

#### Answer

> blablabla

#### Question 1.2

> So the question now is to transform the data in order to get a linear relationship.

#### Answer

> Multiplicative Learning Model:

$$
Y = AX^b
$$

> We want a linear relationship, thus log transformation:

$$
\log(Y) = \log(A) + b*\log(X)
$$

## Data Log Transformation

```{r Data Transformation}

df_log$number_of_solar_panels <- log(df_log$number_of_solar_panels)
df_log$manufacturing_cost <- log(df_log$manufacturing_cost)

```

## Table

```{r Table Data Log}

datatable(round(df_log,2))

```

## Scatterplot

```{r Scatter plot Log}

A_log <- df_log$manufacturing_cost[1]

Plot2 <- ggplot(df_log, aes(x=number_of_solar_panels, y=manufacturing_cost)) + geom_point(color="#32A287") + theme_minimal() + labs(title = "Experience Curve - Log Data",subtitle = "Scatterplot", x="log(Number of Solar Panels)", y="log(Manufacturing Cost)", caption = "Data: Case_Study_1.csv")

Plot2

```

# Exercise 2

## Linear Regression

### Model

> Log-Log Model with No Fixed Intercept

```{r Linear Regression Free Intercept}

fit_2 <- lm(df_log$manufacturing_cost  ~ ., df_log)

intercept_log_2 <- fit_2$coefficients[1]
b_log_2 <- fit_2$coefficients[2]

summ(fit_2)

```

#### Question 2.1

> Interpret the results of the regression analysis

#### Answer

> -   R²: The coefficient of determination, which indicates the proportion of the variance in the dependent variable that can be explained by the independent variable(s). In this case, **95%** of the variance in manufacturing cost can be explained by the number of solar panels used.
>
> -   Adj. R²: The adjusted R² value, which takes into account the number of independent variables in the model. It is similar to R² but penalizes the addition of extraneous variables that do not improve the model's explanatory power.
>
> -   Standard errors: The standard error of the estimate (SE) is a measure of the precision of the regression estimates. It indicates the average amount that the actual manufacturing costs differ from the predicted values based on the regression equation.
>
> -   Intercept: The estimated constant value of the dependent variable when the independent variable is zero. In this case, the intercept is **7.85** which corresponds to **2565.734** in correct scale, that would be the cost of the first unit produced.
>
> -   number_of_solar_panels: The estimated slope coefficient, indicating how much the dependent variable changes for a increase in the independent variable in %. The slope coefficient is **-0.15**, that would mean the cost of manufacturing decrease by **15%** when we increase the number of panels produced by **1%**.
>
> -   t-value: The t-statistic is a measure of the significance of the regression coefficients. It tests the hypothesis that the true coefficient is zero.
>
> -   p-value: The p-value is the probability of obtaining the observed t-value (or more extreme) under the null hypothesis that the true coefficient is zero. A low p-value indicates that the coefficient is statistically significant and can be considered as a predictor of the dependent variable. In this case, both the intercept and the number of solar panels are significant predictors of manufacturing cost, with p-values of 0.00.

#### Questions 2.2

> How well does the model explain the data?

#### Answer

> Since R² and Adj. R² (over 0.7), as well as p-values (under 0.05) of our estimates are all statistically significants, we can say that our model is a good fit to our log data.

### Plot

```{r Log-Linear Regression 2}

Plot3 <- ggplot(df_log, aes(x=number_of_solar_panels, y=manufacturing_cost)) + geom_point(color="#32A287") + theme_minimal() + labs(title = "Experience Curve - Log Data",subtitle = "Linear Regression", x="log(Number of Solar Panels)", y="log(Manufacturing Cost)", caption = "Data: Case_Study_1.csv") + geom_abline(intercept = intercept_log_2, slope = b_log_2, color="#F0C987", size=1) 

Plot3

```

### Homoskedasticity

```{r Homoskedasticity Check}

Residuals_Plot <- ggplot(fit_2, aes(x = .fitted, y = .resid)) +
  geom_point(color="#32A287") +
  geom_hline(yintercept = 0, color ="#F0C987") + theme_minimal() + labs(title = "Homoskedasticity Check",subtitle = "Residuals", x="Fitted Values", y="Residuals", caption = "Data: Case_Study_1.csv")

Residuals_Plot

```

> In the case of our model, based on the residual plot, there is no visible pattern or trend in the residuals, and they appear to be scattered randomly around the horizontal line at zero. This suggests that the assumption of homoskedasticity holds, and there is no evidence of heteroskedasticity in the model.
>
> Therefore, we can conclude that the model satisfies the assumption of homoskedasticity, which is an important assumption for obtaining unbiased and efficient estimates of the regression coefficients and standard errors.

## Learning Rate

### Formula

$$ Formula $$

```{r Learning Rate}

Learning_Rate <- -100*fit_2$coefficients[2]

print(paste("The Learning Rate is",round(Learning_Rate,2),"%"))

```

## Plot to Original Data

```{r Plot with Estimated Coefficient}

X_m <- seq(0,2200,1)

Y_m <- exp(fit_2$coefficients[1])*X_m^(fit_2$coefficients[2])

Multiplicate_Model <- data.frame(X_m,Y_m)
Multiplicate_Model$Y_m[1] <- exp(fit_2$coefficients[1])

colnames(Multiplicate_Model)[1] <- "number_of_solar_panels"
colnames(Multiplicate_Model)[2] <- "manufacturing_cost"

Plot_5 <- ggplot(df, aes(x=number_of_solar_panels, y=manufacturing_cost)) + geom_point(color="#32A287") + theme_minimal() + labs(title = "Experience Curve",subtitle = "Multiplicative Model - Parameters From Log-Log Linear Regression", x="Number of Solar Panels", y="Manufacturing Cost", caption = "Data: Case_Study_1.csv") 
 
Plot_5 + geom_line(Multiplicate_Model, mapping=aes(number_of_solar_panels, manufacturing_cost), color="#F0C987")

```

# Exercise 3

#### Question 3.1

> Estimate the average production cost per solar panel for 4700, 4800, 4900 and 5000 units and compute their mean.

## Predict Future Costs of Solar Panels

```{r Predict}

values <- seq(2200, 5000, 100)

targets <- data.frame(number_of_solar_panels = values)

targets$number_of_solar_panels <- log(targets$number_of_solar_panels)

predictions <- predict.lm(fit_2, newdata = targets)

predictions_final <- exp(predictions)

PREDICTIONS <- data.frame(values, predictions_final)

colnames(PREDICTIONS)[1] ="number_of_solar_panels"
colnames(PREDICTIONS)[2] ="manufacturing_cost"

PREDICTIONS_FINAL <- rbind(df_original,PREDICTIONS)

x_range <- c(0, 2200)

PREDICTIONS_FINAL$color_range <- ifelse(PREDICTIONS_FINAL$number_of_solar_panels >= x_range[1] & PREDICTIONS_FINAL$number_of_solar_panels <= x_range[2], "Data", "Predictions")

Plot3 <- ggplot(PREDICTIONS_FINAL, aes(x=number_of_solar_panels, y=manufacturing_cost, color = PREDICTIONS_FINAL$color_range)) + geom_point() + scale_color_manual(values = c("Data" = "#32A287", "Predictions" = "#F0C987")) + theme_minimal()+ labs(title = "Experience Curve",subtitle = "Predictions From Linear Regression (Log to Standard)", x="Number of Solar Panels", y="Manufacturing Cost", caption = "Data: Case_Study_1.csv", color = "") 

X_m <- seq(0,2200,1)

Y_m <- exp(fit_2$coefficients[1])*X_m^(fit_2$coefficients[2])

Multiplicate_Model <- data.frame(X_m,Y_m)
Multiplicate_Model$Y_m[1] <- exp(fit_2$coefficients[1])

colnames(Multiplicate_Model)[1] <- "number_of_solar_panels"
colnames(Multiplicate_Model)[2] <- "manufacturing_cost"

Plot3 + geom_line(Multiplicate_Model, mapping=aes(number_of_solar_panels, manufacturing_cost), color="#F0C987")

```

## Table Of Future Costs of Solar Panels

```{r Table Of Predictions}

datatable(round(PREDICTIONS_FINAL[PREDICTIONS_FINAL$number_of_solar_panels %in% c(4700, 4800, 4900, 5000), c(1,2)], 2))

```

## Mean Cost Future Solar Panels

```{r Mean Of Predictions}

Targets_1 <- c(4700, 4800, 4900, 5000)

Targets_Costs <- PREDICTIONS_FINAL$manufacturing_cost[PREDICTIONS_FINAL$number_of_solar_panels %in% Targets_1]

Target_Mean <- mean(Targets_Costs)

print(paste("The Mean Cost of Future Productions of Solar Panels is",round(Target_Mean,2)))

```

# Exercise 4

## Confidence Interval of b

```{r Confidence Interval}

Confidence_95_b <- confint(fit_2, 2, level = 0.95)

Confidence_95_b 

Confidence_95_intercept <- confint(fit_2, 1, level = 0.95)

Confidence_95_intercept

b_upper <- Confidence_95_b[1,1]

b_lower <- Confidence_95_b[1,2]

intercept_upper <- Confidence_95_intercept [1,1]

intercept_lower <- Confidence_95_intercept[1,2]

reg_upper <- intercept_upper + b_upper * df$number_of_solar_panels

reg_lower <- intercept_lower + b_lower * df$number_of_solar_panels

```

## Linear Regression with Confidence Interval

```{r Linear Regression Interval}






```

# References

[What Is The Experience Curve And Why It Matters In Business](https://fourweekmba.com/experience-curve/)

[Experience And Learning Curves](https://www.referenceforbusiness.com/management/Em-Exp/Experience-and-Learning-Curves.html#:~:text=By%20convention%2C%20we%20refer%20to,cost%20of%20direct%20labor%20hours)

[Riding The Experience Curve](https://asc.army.mil/web/news-alt-jas17-riding-the-experience-curve/)

[BILL BROOKFIELD Management Accounting](http://www.cimaglobal.com/documents/importeddocuments/fm_april_05_p44-47.pdf)
